{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"http://www.reddit.com/r/boardgames.json\"\n",
    "\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t3_9ee99l'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = res.json()\n",
    "\n",
    "data['data']['after']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### and save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "after =  t3_9ee99l\n",
      "200\n",
      "after =  t3_9eb8vn\n",
      "200\n",
      "after =  t3_9e6xw6\n",
      "200\n",
      "after =  t3_9e2s8w\n",
      "200\n",
      "after =  t3_9dw6ae\n",
      "200\n",
      "after =  t3_9dqdct\n",
      "200\n",
      "after =  t3_9df4zm\n",
      "200\n",
      "after =  t3_9de9il\n",
      "200\n",
      "after =  t3_9davlb\n",
      "200\n",
      "after =  t3_9cub5g\n",
      "200\n",
      "after =  t3_9cvytt\n",
      "200\n",
      "after =  t3_9cyc9j\n",
      "200\n",
      "after =  t3_9csyye\n",
      "200\n",
      "after =  t3_9cgoa0\n",
      "200\n",
      "after =  t3_9cbiai\n",
      "200\n",
      "after =  t3_9c411f\n",
      "200\n",
      "after =  t3_9bvh8u\n",
      "200\n",
      "after =  t3_9bvn66\n",
      "200\n",
      "after =  t3_9bpo35\n",
      "200\n",
      "after =  t3_9blekr\n",
      "200\n",
      "after =  t3_9b9snk\n",
      "200\n",
      "after =  t3_9bdryi\n",
      "200\n",
      "after =  t3_9b9zoj\n",
      "200\n",
      "after =  t3_9b30yc\n",
      "200\n",
      "after =  t3_9asvbt\n",
      "200\n",
      "after =  t3_9avwxj\n",
      "200\n",
      "after =  t3_9af2bk\n",
      "200\n",
      "after =  t3_9ahnbr\n",
      "200\n",
      "after =  t3_9aa8m9\n",
      "200\n",
      "after =  t3_9a26az\n",
      "200\n",
      "after =  t3_9a1nm8\n",
      "200\n",
      "after =  t3_99ymb1\n",
      "200\n",
      "after =  t3_99qhpm\n",
      "200\n",
      "after =  t3_99m2oy\n",
      "200\n",
      "after =  t3_99h30v\n",
      "200\n",
      "after =  t3_99csgu\n",
      "200\n",
      "after =  t3_995aqj\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "url = \"http://www.reddit.com/r/boardgames.json\"\n",
    "headers = {'User-agent': 'Li-Zhong'}\n",
    "params = {}\n",
    "all_posts = []\n",
    "\n",
    "for _ in range(50):\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    res.raise_for_status()\n",
    "    print(res.status_code)\n",
    "    if res.status_code == 200:\n",
    "        data = res.json() \n",
    "        all_posts += data['data']['children']\n",
    "        after = data['data']['after']\n",
    "        if after == None:\n",
    "            break\n",
    "        else:\n",
    "            params['after'] = after\n",
    "            print(\"after = \", params['after'])\n",
    "\n",
    "    elif res == 404:\n",
    "        print('404 error: not found')\n",
    "        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "940"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_posts).to_csv(\"./list_of_posts_boardgame.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "* Scrape books subrredit from reddits.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "after =  t3_9dtqyi\n",
      "t3_9dtqyi\n",
      "200\n",
      "after =  t3_9dhck4\n",
      "t3_9dhck4\n",
      "200\n",
      "after =  t3_9cys3t\n",
      "t3_9cys3t\n",
      "200\n",
      "after =  t3_9bvjjn\n",
      "t3_9bvjjn\n",
      "200\n",
      "after =  t3_9bq74l\n",
      "t3_9bq74l\n",
      "200\n",
      "after =  t3_9b6fmg\n",
      "t3_9b6fmg\n",
      "200\n",
      "after =  t3_9b1z1s\n",
      "t3_9b1z1s\n",
      "200\n",
      "after =  t3_9aqeu0\n",
      "t3_9aqeu0\n",
      "200\n",
      "after =  t3_9a7x41\n",
      "t3_9a7x41\n",
      "200\n",
      "after =  t3_99b6yl\n",
      "t3_99b6yl\n",
      "200\n",
      "after =  t3_992j50\n",
      "t3_992j50\n",
      "200\n",
      "after =  t3_98ps5h\n",
      "t3_98ps5h\n",
      "200\n",
      "after =  t3_981wkf\n",
      "t3_981wkf\n",
      "200\n",
      "after =  t3_97lrmd\n",
      "t3_97lrmd\n",
      "200\n",
      "after =  t3_971emh\n",
      "t3_971emh\n",
      "200\n",
      "after =  t3_970asq\n",
      "t3_970asq\n",
      "200\n",
      "after =  t3_96b43t\n",
      "t3_96b43t\n",
      "200\n",
      "after =  t3_962qbn\n",
      "t3_962qbn\n",
      "200\n",
      "after =  t3_95j3me\n",
      "t3_95j3me\n",
      "200\n",
      "after =  t3_94y87f\n",
      "t3_94y87f\n",
      "200\n",
      "after =  t3_9530gf\n",
      "t3_9530gf\n",
      "200\n",
      "after =  t3_94bvaj\n",
      "t3_94bvaj\n",
      "200\n",
      "after =  t3_94220b\n",
      "t3_94220b\n",
      "200\n",
      "after =  t3_93u7e4\n",
      "t3_93u7e4\n",
      "200\n",
      "after =  t3_939hyc\n",
      "t3_939hyc\n",
      "200\n",
      "after =  t3_9307ra\n",
      "t3_9307ra\n",
      "200\n",
      "after =  t3_92nuy3\n",
      "t3_92nuy3\n",
      "200\n",
      "after =  t3_92lch2\n",
      "t3_92lch2\n",
      "200\n",
      "after =  t3_91sjdl\n",
      "t3_91sjdl\n",
      "200\n",
      "after =  t3_91lcsx\n",
      "t3_91lcsx\n",
      "200\n",
      "after =  t3_90xfh2\n",
      "t3_90xfh2\n",
      "200\n",
      "after =  t3_90qm9b\n",
      "t3_90qm9b\n",
      "200\n",
      "after =  t3_907j3n\n",
      "t3_907j3n\n",
      "200\n",
      "after =  t3_8zurq5\n",
      "t3_8zurq5\n",
      "200\n",
      "after =  t3_8ytqxe\n",
      "t3_8ytqxe\n",
      "200\n",
      "after =  t3_8ym5xe\n",
      "t3_8ym5xe\n",
      "200\n",
      "after =  t3_8y95vu\n",
      "t3_8y95vu\n",
      "200\n",
      "after =  t3_8xp9t8\n",
      "t3_8xp9t8\n",
      "200\n",
      "after =  None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "url_2 = \"http://www.reddit.com/r/TrueReddit.json\"\n",
    "headers = {'User-agent': 'Li-Zhong'}\n",
    "params = {}\n",
    "all_posts_2 = []\n",
    "\n",
    "for _ in range(50):\n",
    "    res_2 = requests.get(url_2, headers=headers, params=params)\n",
    "    res_2.raise_for_status()\n",
    "    print(res_2.status_code)\n",
    "    if res_2.status_code == 200:\n",
    "        data_2 = res_2.json() \n",
    "        all_posts_2 += data_2['data']['children']\n",
    "        after = data_2['data']['after']\n",
    "        print(\"after = \", after)\n",
    "        if after == None:\n",
    "            break\n",
    "        else:\n",
    "            params['after'] = after\n",
    "            print(params['after'])\n",
    "\n",
    "    elif res_2 == 404:\n",
    "        print('404 error: not found')\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url=url_2,headers=headers,params={'after':'t3_8vhmb1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.json()['data']['after']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "963"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_posts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_posts_2).to_csv(\"./list_of_posts_true_reddit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "focus": false,
    "id": "783fd153-28ac-47ab-bfca-27e7c1de95b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:6: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "list_of_posts_board = pd.DataFrame.from_csv(\"./list_of_posts_boardgame.csv\")\n",
    "\n",
    "list_of_posts_truered = pd.DataFrame.from_csv(\"./list_of_posts_true_reddit.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(940, 963)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_posts_board), len(list_of_posts_truered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_posts_board_extract = []\n",
    "for post in list_of_posts_board['data'][1:]:\n",
    "    post = ast.literal_eval(post)\n",
    "    if post['selftext'] == '':\n",
    "        continue\n",
    "    else:\n",
    "        list_of_posts_board_extract.append(post['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "812"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_posts_board_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**Welcome to /r/boardgames Daily Discussion an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love board games and have a small collection...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I read the rules, I watch a game play video bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love the game and am finally looking at gett...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been playing these two games a lot lately...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  class\n",
       "0  **Welcome to /r/boardgames Daily Discussion an...      1\n",
       "1  I love board games and have a small collection...      1\n",
       "2  I read the rules, I watch a game play video bu...      1\n",
       "3  I love the game and am finally looking at gett...      1\n",
       "4  I've been playing these two games a lot lately...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_board = pd.DataFrame(list_of_posts_board_extract, columns=['post'])\n",
    "pd_board['class'] = 1\n",
    "pd_board.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_posts_truered_extract = []\n",
    "for post in list_of_posts_truered['data'][1:]:\n",
    "    post = ast.literal_eval(post)\n",
    "#    post_ext = {}\n",
    "    if post['title'] == '':\n",
    "        continue\n",
    "    else:\n",
    "#        post_ext['selftext'] = post['selftext']\n",
    "        \n",
    "        list_of_posts_truered_extract.append(post['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_posts_truered_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Billionaires v teachers: the Koch brothers' pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jose Manuel Martinez — a doting grandfather kn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Electric Scooters Are Reshaping Cities</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor Swift and the Cult of Early Success</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Richard Sackler became a multi-billionaire aft...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  class\n",
       "0  Billionaires v teachers: the Koch brothers' pl...      0\n",
       "1  Jose Manuel Martinez — a doting grandfather kn...      0\n",
       "2         How Electric Scooters Are Reshaping Cities      0\n",
       "3         Taylor Swift and the Cult of Early Success      0\n",
       "4  Richard Sackler became a multi-billionaire aft...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_truered = pd.DataFrame(list_of_posts_truered_extract, columns=['post'])\n",
    "pd_truered['class'] = 0\n",
    "pd_truered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two lists together\n",
    "list_concat = pd.concat([pd_board, pd_truered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**Welcome to /r/boardgames Daily Discussion an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love board games and have a small collection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I read the rules, I watch a game play video bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love the game and am finally looking at gett...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been playing these two games a lot lately...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post\n",
       "0  **Welcome to /r/boardgames Daily Discussion an...\n",
       "1  I love board games and have a small collection...\n",
       "2  I read the rules, I watch a game play video bu...\n",
       "3  I love the game and am finally looking at gett...\n",
       "4  I've been playing these two games a lot lately..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_concat.drop('class', axis = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1774, 2)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**Welcome to /r/boardgames Daily Discussion an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love board games and have a small collection...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I read the rules, I watch a game play video bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love the game and am finally looking at gett...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've been playing these two games a lot lately...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hey guys. Looking to get into either of these ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[Here is the mission card](https://ksr-ugc.img...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://i.imgur.com/ac4WAMK.jpg\\n(Sorry for po...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>One of the minor secrets, the MultiPass, reads...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I'm not in a habit of sleeving my cards so for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  class\n",
       "0  **Welcome to /r/boardgames Daily Discussion an...      1\n",
       "1  I love board games and have a small collection...      1\n",
       "2  I read the rules, I watch a game play video bu...      1\n",
       "3  I love the game and am finally looking at gett...      1\n",
       "4  I've been playing these two games a lot lately...      1\n",
       "5  Hey guys. Looking to get into either of these ...      1\n",
       "6  [Here is the mission card](https://ksr-ugc.img...      1\n",
       "7  https://i.imgur.com/ac4WAMK.jpg\\n(Sorry for po...      1\n",
       "8  One of the minor secrets, the MultiPass, reads...      1\n",
       "9  I'm not in a habit of sleeving my cards so for...      1"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_concat.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cvec = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_trans = cvec.fit_transform(list_concat['post'])\n",
    "tfidf_trans = tfidf.fit_transform(list_concat['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cvec = pd.DataFrame(cvec_trans.todense(), columns = cvec.get_feature_names())\n",
    "X_tfidf = pd.DataFrame(tfidf_trans.todense(), columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atlantis</th>\n",
       "      <th>atlas</th>\n",
       "      <th>atleast</th>\n",
       "      <th>atmosphere</th>\n",
       "      <th>atmospheric</th>\n",
       "      <th>atomwaffen</th>\n",
       "      <th>atrocities</th>\n",
       "      <th>attached</th>\n",
       "      <th>attachment</th>\n",
       "      <th>attack</th>\n",
       "      <th>...</th>\n",
       "      <th>delve</th>\n",
       "      <th>delvers</th>\n",
       "      <th>delving</th>\n",
       "      <th>demand</th>\n",
       "      <th>demands</th>\n",
       "      <th>dementia</th>\n",
       "      <th>demigods</th>\n",
       "      <th>demo</th>\n",
       "      <th>democracy</th>\n",
       "      <th>democratic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   atlantis  atlas  atleast  atmosphere  atmospheric  atomwaffen  atrocities  \\\n",
       "0         0      0        0           0            0           0           0   \n",
       "1         0      0        0           0            0           0           0   \n",
       "2         0      0        0           0            0           0           0   \n",
       "3         0      0        0           0            0           0           0   \n",
       "4         0      0        0           0            0           0           0   \n",
       "\n",
       "   attached  attachment  attack     ...      delve  delvers  delving  demand  \\\n",
       "0         0           0       0     ...          0        0        0       0   \n",
       "1         0           0       0     ...          0        0        0       0   \n",
       "2         0           0       0     ...          0        0        0       0   \n",
       "3         0           0       0     ...          0        0        0       0   \n",
       "4         0           0       0     ...          0        0        0       0   \n",
       "\n",
       "   demands  dementia  demigods  demo  democracy  democratic  \n",
       "0        0         0         0     0          0           0  \n",
       "1        0         0         0     0          0           0  \n",
       "2        0         0         0     0          0           0  \n",
       "3        0         0         0     0          0           0  \n",
       "4        0         0         0     0          0           0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cvec.iloc[:,2000:4000].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atlantis</th>\n",
       "      <th>atlas</th>\n",
       "      <th>atleast</th>\n",
       "      <th>atmosphere</th>\n",
       "      <th>atmospheric</th>\n",
       "      <th>atomwaffen</th>\n",
       "      <th>atrocities</th>\n",
       "      <th>attached</th>\n",
       "      <th>attachment</th>\n",
       "      <th>attack</th>\n",
       "      <th>...</th>\n",
       "      <th>delve</th>\n",
       "      <th>delvers</th>\n",
       "      <th>delving</th>\n",
       "      <th>demand</th>\n",
       "      <th>demands</th>\n",
       "      <th>dementia</th>\n",
       "      <th>demigods</th>\n",
       "      <th>demo</th>\n",
       "      <th>democracy</th>\n",
       "      <th>democratic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   atlantis  atlas  atleast  atmosphere  atmospheric  atomwaffen  atrocities  \\\n",
       "0       0.0    0.0      0.0         0.0          0.0         0.0         0.0   \n",
       "1       0.0    0.0      0.0         0.0          0.0         0.0         0.0   \n",
       "2       0.0    0.0      0.0         0.0          0.0         0.0         0.0   \n",
       "3       0.0    0.0      0.0         0.0          0.0         0.0         0.0   \n",
       "4       0.0    0.0      0.0         0.0          0.0         0.0         0.0   \n",
       "\n",
       "   attached  attachment  attack     ...      delve  delvers  delving  demand  \\\n",
       "0       0.0         0.0     0.0     ...        0.0      0.0      0.0     0.0   \n",
       "1       0.0         0.0     0.0     ...        0.0      0.0      0.0     0.0   \n",
       "2       0.0         0.0     0.0     ...        0.0      0.0      0.0     0.0   \n",
       "3       0.0         0.0     0.0     ...        0.0      0.0      0.0     0.0   \n",
       "4       0.0         0.0     0.0     ...        0.0      0.0      0.0     0.0   \n",
       "\n",
       "   demands  dementia  demigods  demo  democracy  democratic  \n",
       "0      0.0       0.0       0.0   0.0        0.0         0.0  \n",
       "1      0.0       0.0       0.0   0.0        0.0         0.0  \n",
       "2      0.0       0.0       0.0   0.0        0.0         0.0  \n",
       "3      0.0       0.0       0.0   0.0        0.0         0.0  \n",
       "4      0.0       0.0       0.0   0.0        0.0         0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf.iloc[:,2000:4000].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "#### Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1774, 13170), (1774, 13170), (1774, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cvec.shape, X_tfidf.shape, list_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cvec,list_concat['class'], test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X_tfidf,list_concat['class'], test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    962\n",
       "1    812\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_concat['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For this particular dataset, our baseline accuracy should be proportional to our data blance: 969/(969+811) = 54.4%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "4fb29de2-5b98-474c-a4ad-5170b72b9aea"
   },
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy score:  0.997\n",
      "test accuracy score:  0.9392\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_train_predict = rf.predict(X_train)\n",
    "print(\"train accuracy score: \", rf.score(X_train, y_train).round(4))\n",
    "print(\"test accuracy score: \", rf.score(X_test, y_test).round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy score:  0.997\n",
      "test accuracy score:  0.9234\n"
     ]
    }
   ],
   "source": [
    "rf1 = RandomForestClassifier(random_state=42)\n",
    "rf1.fit(X1_train, y1_train)\n",
    "y1_train_predict = rf1.predict(X1_train)\n",
    "print(\"train accuracy score: \", rf1.score(X1_train, y1_train).round(4))\n",
    "print(\"test accuracy score: \", rf1.score(X1_test, y1_test).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      1.00      0.95       237\n",
      "          1       0.99      0.87      0.93       207\n",
      "\n",
      "avg / total       0.94      0.94      0.94       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_scores_rf = cross_val_score(RandomForestClassifier(random_state=42), X_train, y_train)\n",
    "print(cross_scores_rf.mean().round(4))\n",
    "\n",
    "y_test_predict = rf.predict(X_test)\n",
    "report_test = classification_report(y_test, y_test_predict)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9316\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      1.00      0.93       237\n",
      "          1       0.99      0.84      0.91       207\n",
      "\n",
      "avg / total       0.93      0.92      0.92       444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_scores_rf1 = cross_val_score(RandomForestClassifier(random_state=42), X1_train, y1_train)\n",
    "print(cross_scores_rf1.mean().round(4))\n",
    "\n",
    "y1_test_predict = rf1.predict(X1_test)\n",
    "report_test1 = classification_report(y1_test, y1_test_predict)\n",
    "print(report_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CountVectorizer and Tfidf create similar results and Tfidf vectorizer is sightly better in recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "\n",
    "score_function = make_scorer(accuracy_score)\n",
    "params = {\n",
    "    \"cvec__stop_words\"  : [None, 'english'],\n",
    "    \"cvec__ngram_range\" : [(1,1), (1,2)],\n",
    "    \"rf__n_estimators\"  : [40, 60, 80],\n",
    "    \"rf__criterion\"     : [\"gini\", \"entropy\"],\n",
    "    \"rf__max_depth\"     : [30, 50, 70],\n",
    "    \"rf__random_state\"  : [42]\n",
    "             }\n",
    "steps = [('cvec', CountVectorizer()), ('rf', RandomForestClassifier())]\n",
    "pipe = Pipeline(steps = steps)\n",
    "gs = GridSearchCV(pipe, param_grid = params , scoring = score_function, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(list_concat['post'], list_concat['class'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 216 out of 216 | elapsed:  2.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'cvec__stop_words': [None, 'english'], 'cvec__ngram_range': [(1, 1), (1, 2)], 'rf__n_estimators': [40, 60, 80], 'rf__criterion': ['gini', 'entropy'], 'rf__max_depth': [30, 50, 70], 'rf__random_state': [42]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(accuracy_score), verbose=1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train_rf, y_train_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': None,\n",
       " 'rf__criterion': 'gini',\n",
       " 'rf__max_depth': 50,\n",
       " 'rf__n_estimators': 60,\n",
       " 'rf__random_state': 42}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9678"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9512"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_rf, y_test_rf).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96       294\n",
      "          1       0.98      0.91      0.94       239\n",
      "\n",
      "avg / total       0.95      0.95      0.95       533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = gs.predict(X_test_rf)\n",
    "report_test = classification_report(y_test_rf, y_test_predict)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_function = make_scorer(accuracy_score)\n",
    "params_lr = {\n",
    "    \"cvec__stop_words\"  : [None, 'english'],\n",
    "    \"cvec__max_df\"      : [1.0, 3.0],\n",
    "    \"cvec__ngram_range\" : [(1,1), (1,2)],\n",
    "    \"cvec__lowercase\"   : ['True', 'False'],\n",
    "    \"logic__C\"  : [0.1 ,1 , 5],\n",
    "    \"logic__max_iter\"     : [5, 10, 20],\n",
    "    \"logic__random_state\"  : [42]\n",
    "             }\n",
    "steps_lr = [('cvec', CountVectorizer()), ('logic', LogisticRegression())]\n",
    "pipe_lr = Pipeline(steps = steps_lr)\n",
    "gs_lr = GridSearchCV(pipe_lr, param_grid = params_lr , scoring = score_function, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(list_concat['post'], list_concat['class'], test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 432 out of 432 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('cvec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'cvec__stop_words': [None, 'english'], 'cvec__max_df': [1.0, 3.0], 'cvec__ngram_range': [(1, 1), (1, 2)], 'cvec__lowercase': ['True', 'False'], 'logic__C': [0.1, 1, 5], 'logic__max_iter': [5, 10, 20], 'logic__random_state': [42]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(accuracy_score), verbose=1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__lowercase': 'True',\n",
       " 'cvec__max_df': 1.0,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__stop_words': None,\n",
       " 'logic__C': 1,\n",
       " 'logic__max_iter': 10,\n",
       " 'logic__random_state': 42}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9686"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.best_score_.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.score(X_test_lr, y_test_lr).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.99      0.96       294\n",
      "          1       0.98      0.91      0.94       239\n",
      "\n",
      "avg / total       0.95      0.95      0.95       533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = gs.predict(X_test_lr)\n",
    "report_test = classification_report(y_test_lr, y_test_predict)\n",
    "print(report_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify your Subreddit Post by Machine Learning!\n",
    "\n",
    "There are hundreds and thousands of posts per day on reddit.com and we definitely want computers to be able to “recognize” each post by its subreddit category for multiple usages!\n",
    "But how?\n",
    "\n",
    "Machine learning algorithms can easily help! In this project, we showcase how this is done by machine learning. Two popular kinds of subreddit posts are scrapped from reddit.com: Boardgames and True reddits. NLP is used next to analyze the word frequency for each post and a model is built on top of that to predict which category the post belongs to. The parameters are fine tuned later to optimize results. 96 of 100 posts can be categorized correctly.\n",
    "\n",
    "I believe there are more we can work to expand the results to a more diverse and in depth application. \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
